---
title: "Travel Audience Test"
output: 
  html_notebook:
    toc: true
    number_sections: true
---

# Preparations

First let's load some general purpose packages.

```{r, message=FALSE}
require(tidyverse)
require(lubridate)
require(knitr)
```

Data size is small enough to be loaded into memory as "data frame" object in R. This has similar properties to Python's pandas DF. 

```{r, message=FALSE}
path = 'C:\\Users\\yizhart\\Downloads\\Temp\\Amadeus'
df <- read_csv(file = paste0(path, '\\logs.csv'), progress = FALSE)
df %>% 
  head(5) %>% 
  kable()
```

# Calculating Features

## The ```highly_active``` feature

In the absence of an "objective" measure or threshold to describe an "active" customer we must rely on relative ranking using the following steps:

### Count per-user impressions

We count the number of impressions per customer during the entire month. The mose straight forward way is to group by ```uuid``` and count rows:

```{r}
df_users <- df %>% 
  group_by(uuid) %>% 
  summarize(imp_count = n())

df_users %>% head(5) %>% kable()
```

### Establishing a threshold for ```highly_active``` definition

The summary statistics of the impression count shows that the data probably contain an outlier group (a good clue is the face that the mean value is higher than the 75'th percentile). 

```{r}
df_users %>% select(imp_count) %>% summary()
```


This is also clearly visible when looking at a simple histogram:

```{r}
df_users %>% 
  ggplot(aes(x = imp_count, y = ..density..)) + 
    geom_histogram(bins = 100)
    
```

A slightly different perpecitve (looking at a logarithmic scale) reveals that this is indeed a very small, but definitly no a single outlier. There does not seem to be a "natural" threshold that will clasify users as "highly active".

```{r}
df_users %>% 
  ggplot(aes(x = imp_count)) + 
    stat_ecdf(geom = 'step') + 
    scale_x_log10()
```

I therefore use conventional wisdom and try to set the theshold by quantiles:

```{r}
probs <- c(0.01 * 90:98, 0.001 * 990:1000)

df_users %>% 
  select(imp_count) %>% 
  unlist() %>% 
  quantile(probs = probs)
```

There's still an order of magnitude difference between the max count and even the 99'th percentile. This suggests that the data may contain two distinct phenomena: 

1. Users that are indeed highly active, with impressions count > 10.
2. Suspicious cases (probably data errors, or bots) with impression count > 100.

For the purpose of this exercise I will group the 2 together. Starting from the beginning we can extend the code to include the definition:

```{r}
df_users <- df %>% 
  group_by(uuid) %>% 
  summarize(imp_count = n()) %>% 
  ## Adding the threshold
  mutate(highly_active = imp_count > 10)

df_users %>%
  count(highly_active) %>% 
  ggplot(aes(x = highly_active, y = n)) + geom_bar(stat = 'identity')
```

## The ```multiple_days``` feature

This feature can be calculated by looking only on the date part in the ```ts``` timestamp and counting distinct values: 

```{r}
df_users <- df %>% 
  ## Adding a date only field
  mutate(imp_date = as.Date(ts)) %>% 
  group_by(uuid) %>% 
  summarize(
    imp_count = n(),
    active_days = n_distinct(imp_date)) %>% 
  ## And additng to the summary
  mutate(
    highly_active = imp_count > 10,
    multiple_days = active_days > 1
    )

df_users %>%
  count(multiple_days) %>% 
  ggplot(aes(x = multiple_days, y = n)) + geom_bar(stat = 'identity')
```

## The ```weekday_biz``` feature

I define weekday business hours as Mon - Fri, 08:00 - 17:00. Using a similar approach I calculate for each user the % of impressions that occured in this time window:


```{r}
df_users <- df %>% 
  mutate(
    imp_date = as.Date(ts),
    ## We add the weekday and hour for criteria calculation
    imp_weekday = wday(ts), 
    imp_hour = hour(ts),
    imp_weekday_biz = if_else(
      imp_weekday > 1 & imp_weekday < 7 & imp_hour >= 8 & imp_hour <=17, 1, 0)
    ) %>% 
  group_by(uuid) %>% 
  summarize(
    imp_count = n(),
    active_days = n_distinct(imp_date),
    ## And count impressions    
    weekday_biz_count = sum(imp_weekday_biz)
    ) %>% 
  ## And additng to the summary
  mutate(
    highly_active = imp_count > 10,
    multiple_days = active_days > 1,
    weekday_biz_prop = weekday_biz_count / imp_count,
    weekday_biz = weekday_biz_prop > 0.5
    )

df_users %>%
  ggplot(aes(x = weekday_biz_prop)) + geom_histogram()
```

Unlike the ```highly_active``` feature, it would seem that users are concetrate their activity either in the business hours or outside the business hours (with only a few users splitting their time in between). A reasonable threshould would therefore be 0.5 (was calculated above). 

```{r}
df_users %>% 
  count(weekday_biz) %>% 
  ggplot(aes(x = weekday_biz, y = n)) + geom_bar(stat = 'identity')

```

## The additional feature: ```shared_computer```

An interesting way to gain insight into the way customers engage with a website is to step outside the framework of a single ```uuid``` and try to observe how users relate to one another. In this case the feature I chose to explore is how users are connected to one another via their use of the same IP address.

The first step is to understand how may users use a single IP

```{r}
df_IP <- df %>% 
  group_by(hashed_ip) %>% 
  summarise(
    imp_count = n(),
    user_count = n_distinct(uuid)
  )

df_IP %>%
  select(imp_count, user_count) %>% 
  summary()
```

Focusing on the distribution of user count, it would seem that the majority of IP address are only ever used by a single user:

```{r}
df_IP %>% 
  ggplot(aes(x = user_count)) + 
    stat_ecdf(geom = 'step') + 
    scale_x_log10()
```

We can now match every single impression with the IP information, and see what proprtion of usee impressions was made on shared IPs (with ```user_count``` > 1). The code below describes the complete calculation of all features 


```{r}
df_users <- df %>% 
  mutate(
    imp_date = as.Date(ts),
    imp_weekday = wday(ts), 
    imp_hour = hour(ts),
    imp_weekday_biz = if_else(
      imp_weekday > 1 & imp_weekday < 7 & imp_hour >= 8 & imp_hour <=17, 1, 0)
    ) %>% 
  ## matching to the IP data and identify shared IPs
  left_join(
    df %>% 
      group_by(hashed_ip) %>% 
      summarise(
        IP_imp_count = n(),
        IP_user_count = n_distinct(uuid)
        ),
    by = c('hashed_ip' = 'hashed_ip')
    ) %>% 
  mutate(shared_IP = IP_user_count > 1) %>%
  group_by(uuid) %>% 
  summarize(
    imp_count = n(),
    active_days = n_distinct(imp_date),
    weekday_biz_count = sum(imp_weekday_biz),
    ## aggregating the shared_IP counts
    shared_IP_count = sum(shared_IP)
    ) %>% 
  ## And additng to the summary
  mutate(
    highly_active = imp_count > 10,
    multiple_days = active_days > 1,
    weekday_biz_prop = weekday_biz_count / imp_count,
    weekday_biz = weekday_biz_prop > 0.5,
    shared_IP_prop = shared_IP_count / imp_count,
    shared_IP_user = shared_IP_prop > 0.5
    )

df_users %>% 
  ggplot(aes(shared_IP_prop)) + geom_histogram()
```

Since most user either use a unique IP or a shared IP for all of their imporessions, we can set a threshold for this feature at 0.5 (as calcuated above)

```{r}
df_users %>% 
  count(shared_IP_user) %>% 
  ggplot(aes(x = shared_IP_user, y = n)) + geom_bar(stat = 'identity')
```


## The final code
With a cleanup stage at the end and some documentation in the comments

```{r, echo = TRUE}
df_users <- df %>% 
  ## pre-calculating some features required for later calculations
  mutate(
    ## extract date from the timestamp
    imp_date = as.Date(ts),
    ## Extract day-of-week
    imp_weekday = wday(ts),
    ## Extract hour
    imp_hour = hour(ts),
    ## define "weekday business hour" criteria fora single impression
    imp_weekday_biz = if_else(
      imp_weekday > 1 & imp_weekday < 7 & imp_hour >= 8 & imp_hour <=17, 1, 0)
    ) %>% 
  left_join(
    ## Aggregate impressions by IP and count impressions / distinct users
    df %>% 
      group_by(hashed_ip) %>% 
      summarise(
        IP_imp_count = n(),
        IP_user_count = n_distinct(uuid)
        ),
    by = c('hashed_ip' = 'hashed_ip')
    ) %>% 
  ## define the criteria for a shared IP (>1 uuid)
  mutate(shared_IP = IP_user_count > 1) %>%
  ## Aggregate impressions by uuid 
  group_by(uuid) %>% 
  summarize(
    imp_count = n(),
    active_days = n_distinct(imp_date),
    weekday_biz_count = sum(imp_weekday_biz),
    shared_IP_count = sum(shared_IP)
    ) %>% 
  ## Calcuate thresholds as set by previous analysis
  mutate(
    highly_active = imp_count > 10,
    multiple_days = active_days > 1,
    weekday_biz_prop = weekday_biz_count / imp_count,
    weekday_biz = weekday_biz_prop > 0.5,
    shared_IP_prop = shared_IP_count / imp_count,
    shared_IP_user = shared_IP_prop > 0.5
    ) %>% 
  # Cleaup
  select(-imp_count, -active_days, -weekday_biz_count, -shared_IP_count, -weekday_biz_prop, -shared_IP_prop)

df_users %>% head(5) %>% kable()
```


# A note on working with larger datasets

To allow more efficient processing of the feature extraction process (and to also demonstrate how the following actions can be scaled up to Spark / Hadoop machines) the first step is to partition the data (or "nest" in R terminology). Since we are not going to explore cross-user features at this time, we can partition data by ```uuid```

```{r}
df_user <- df %>% nest(-uuid)
dim(df_user)
```

The new data structure contains the key ```uuid``` and a list of data frames (one per user).

```{r}
df_user %>% colnames()
```

We can now calculate the same features with "map / reduce" approach that can be easily distributed and parallelized. For example, counting impressions per user:

```{r}
df_user <- df_user %>% 
  mutate(
    imp_count = map_int(.x = data, .f = function(x) dim(x)[1])
    )
df_user %>% select(-data, -uuid) %>% summary()
```
